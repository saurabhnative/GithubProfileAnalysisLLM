{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3889ec-13a7-4819-b0a1-b5e6d65e783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install gradio\n",
    "!pip install matplotlib\n",
    "!pip install plotly\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install wordcloud\n",
    "!pip install -q accelerate\n",
    "!pip install -q requests\n",
    "!pip install -q protobuf\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49204d5f-ca0c-4bb6-9625-aaab629d2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9962ca-89cd-4f57-870a-090dfe042c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b05384-b723-4dfe-ba12-6624fec71821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Profile Analyzer\n",
    "# This notebook analyzes GitHub profiles\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import io\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define GitHub API functions\n",
    "def get_user_info(username):\n",
    "    \"\"\"Fetch GitHub user profile information\"\"\"\n",
    "    url = f\"https://api.github.com/users/{username}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": f\"Failed to fetch user data: {response.status_code}\"}\n",
    "\n",
    "def get_user_repos(username):\n",
    "    \"\"\"Fetch GitHub user repositories\"\"\"\n",
    "    url = f\"https://api.github.com/users/{username}/repos?per_page=100&sort=updated\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": f\"Failed to fetch repositories: {response.status_code}\"}\n",
    "\n",
    "def get_repo_languages(repo_url):\n",
    "    \"\"\"Fetch languages used in a repository\"\"\"\n",
    "    response = requests.get(repo_url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "# Load model\n",
    "def load_model():\n",
    "    print(\"Loading model...\")\n",
    "\n",
    "    model_id = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "    # Check if user is in Colab to apply optimizations\n",
    "    try:\n",
    "        import google.colab\n",
    "        is_colab = True\n",
    "    except:\n",
    "        is_colab = False\n",
    "\n",
    "    # First load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ.get(\"HF_TOKEN\"))\n",
    "\n",
    "    # Configure model loading with optimizations\n",
    "    load_config = {\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "        \"device_map\": \"auto\"\n",
    "    }\n",
    "\n",
    "    if is_colab:\n",
    "        # Further optimizations for Colab environment\n",
    "        print(\"Running in Colab - applying additional optimizations\")\n",
    "\n",
    "    # Load the model with optimizations\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        token=os.environ.get(\"HF_TOKEN\"),\n",
    "        **load_config\n",
    "    )\n",
    "\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to generate text with the loaded model\n",
    "def generate_with_model(model, tokenizer, prompt, max_length=2048):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.15\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    # Extract just the generated part (after the prompt)\n",
    "    generated_text = response[len(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)):]\n",
    "    return generated_text\n",
    "\n",
    "# Create structured data from GitHub API results\n",
    "def prepare_github_data(username):\n",
    "    print(f\"Fetching data for GitHub user: {username}\")\n",
    "\n",
    "    user_data = get_user_info(username)\n",
    "    if \"error\" in user_data:\n",
    "        return f\"Error: {user_data['error']}\"\n",
    "\n",
    "    repos_data = get_user_repos(username)\n",
    "    if isinstance(repos_data, dict) and \"error\" in repos_data:\n",
    "        return f\"Error: {repos_data['error']}\"\n",
    "\n",
    "    # Process repos data\n",
    "    repos_info = []\n",
    "    languages_count = {}\n",
    "    repo_topics = []\n",
    "    for repo in repos_data:\n",
    "        # Skip forks unless they have significant changes\n",
    "        if repo.get(\"fork\", False) and repo.get(\"stargazers_count\", 0) < 5:\n",
    "            continue\n",
    "\n",
    "        repo_info = {\n",
    "            \"name\": repo[\"name\"],\n",
    "            \"description\": repo.get(\"description\") or \"No description provided\",\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            'created_at': datetime.strptime(repo['created_at'], '%Y-%m-%dT%H:%M:%SZ'),\n",
    "            'updated_at': datetime.strptime(repo['updated_at'], '%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"url\": repo[\"html_url\"],\n",
    "            \"topics\": repo.get(\"topics\", []),\n",
    "            'size': repo['size'],\n",
    "            'language': repo['language'] if repo['language'] else 'Not specified',\n",
    "            'has_issues': repo['has_issues'],\n",
    "            'open_issues': repo['open_issues_count'],\n",
    "            'is_fork': repo['fork'],\n",
    "            'url': repo['html_url']\n",
    "        }\n",
    "\n",
    "        # Count languages\n",
    "        if repo[\"language\"]:\n",
    "            languages_count[repo[\"language\"]] = languages_count.get(repo[\"language\"], 0) + 1\n",
    "\n",
    "        # Collect repo topics\n",
    "        if repo.get(\"topics\"):\n",
    "            repo_topics.extend(repo.get(\"topics\"))\n",
    "        repos_info.append(repo_info)\n",
    "\n",
    "    # Sort repos by stars\n",
    "    repos_info.sort(key=lambda x: x[\"stars\"], reverse=True)\n",
    "\n",
    "    # Format time with GitHub\n",
    "    joined_date = datetime.strptime(user_data[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%B %Y\")\n",
    "\n",
    "    # Calculate topic frequency\n",
    "    topic_frequency = Counter(repo_topics)\n",
    "    # Combine all data\n",
    "    github_data = {\n",
    "        \"profile\": {\n",
    "            \"username\": username,\n",
    "            \"name\": user_data.get(\"name\") or username,\n",
    "            \"bio\": user_data.get(\"bio\") or \"No bio provided\",\n",
    "            \"location\": user_data.get(\"location\") or \"Location not specified\",\n",
    "            \"public_repos\": user_data[\"public_repos\"],\n",
    "            \"followers\": user_data[\"followers\"],\n",
    "            \"following\": user_data[\"following\"],\n",
    "            \"joined_github\": joined_date,\n",
    "            \"avatar_url\": user_data[\"avatar_url\"],\n",
    "            \"profile_url\": user_data[\"html_url\"]\n",
    "        },\n",
    "        \"repositories\": {\n",
    "            \"total_count\": len(repos_info),\n",
    "            \"top_languages\": languages_count,\n",
    "            \"top_repos\": repos_info[:10],\n",
    "            \"topic_frequency\": dict(topic_frequency.most_common(20)),\n",
    "            \"repos_info\": repos_info\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return github_data\n",
    "\n",
    "# Generate prompts for model\n",
    "def create_analysis_prompt(github_data):\n",
    "    profile = github_data[\"profile\"]\n",
    "    repos = github_data[\"repositories\"]\n",
    "\n",
    "    top_langs = sorted(repos[\"top_languages\"].items(), key=lambda x: x[1], reverse=True)\n",
    "    top_langs_str = \", \".join([f\"{lang} ({count} repos)\" for lang, count in top_langs[:5]])\n",
    "\n",
    "    # Create detailed info about top 5 repos\n",
    "    top_repos_details = \"\"\n",
    "    for i, repo in enumerate(repos[\"top_repos\"][:5], 1):\n",
    "        top_repos_details += f\"\"\"\n",
    "Repository #{i}: {repo['name']}\n",
    "- Description: {repo['description']}\n",
    "- Stars: {repo['stars']}\n",
    "- Forks: {repo['forks']}\n",
    "- Language: {repo['language'] or 'Not specified'}\n",
    "- Last updated: {repo['updated_at']}\n",
    "\"\"\"\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"You are a GitHub profile analyst. Based on the following GitHub profile data, provide a comprehensive analysis.\n",
    "\n",
    "Profile Information:\n",
    "- Username: {profile['username']}\n",
    "- Name: {profile['name']}\n",
    "- Bio: {profile['bio']}\n",
    "- Location: {profile['location']}\n",
    "- Public Repositories: {profile['public_repos']}\n",
    "- Followers: {profile['followers']}\n",
    "- Following: {profile['following']}\n",
    "- Joined GitHub: {profile['joined_github']}\n",
    "\n",
    "Repository Information:\n",
    "- Total Repositories: {repos['total_count']}\n",
    "- Top Languages: {top_langs_str}\n",
    "\n",
    "Top Repositories:{top_repos_details}\n",
    "\n",
    "Your task is to analyze this profile and provide:\n",
    "ðŸ§¾ Overview\n",
    "An overview of the developer's GitHub presence in minumum 2-3 sentences\n",
    "\n",
    "ðŸ’» Main techonologies\n",
    "Main technologies and skills based on repositories\n",
    "\n",
    "ðŸ“Š Development patterns\n",
    "\n",
    "ðŸ’ª Strengths\n",
    "List the standout strengths or qualities â€” such as code quality, language versatility, open-source involvement, or strong project portfolios.\n",
    "\n",
    "ðŸ› ï¸ Key Technologies and Skills\n",
    "Highlight the main programming languages, frameworks, or tools the developer uses, based on repositories and top languages.\n",
    "Format your analysis in markdown with clear sections.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Format repository list as markdown with star counts\n",
    "def format_repo_list(github_data):\n",
    "    repos = github_data[\"repositories\"][\"top_repos\"]\n",
    "\n",
    "    # Create a markdown table for repositories\n",
    "    md_table = \"## Top Repositories with Star Count\\n\\n\"\n",
    "    md_table += \"| Repository | Description | Stars | Language |\\n\"\n",
    "    md_table += \"| ---------- | ----------- | ----- | -------- |\\n\"\n",
    "\n",
    "    # Add each repository to the table\n",
    "    for repo in repos[:15]:  # Limit to top 15\n",
    "        name = repo[\"name\"]\n",
    "        url = repo[\"url\"]\n",
    "        desc = repo[\"description\"].replace(\"|\", \"\\\\|\") if repo[\"description\"] else \"No description\"\n",
    "        stars = repo[\"stars\"]\n",
    "        lang = repo[\"language\"] or \"Not specified\"\n",
    "\n",
    "        md_table += f\"| [{name}]({url}) | {desc} | {stars} â­ | {lang} |\\n\"\n",
    "\n",
    "    return md_table\n",
    "\n",
    "# Create visualizations\n",
    "def word_cloud_output(github_data):\n",
    "    # Prepare list to store visualization images\n",
    "    # Get topic data\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    topics = github_data[\"repositories\"][\"topic_frequency\"]\n",
    "    # print(topics)\n",
    "    width= 800\n",
    "    height= 400\n",
    "    max_words= 100\n",
    "    if topics and len(topics) > 3:\n",
    "        # Create word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=800,\n",
    "            height=500,\n",
    "            background_color='white',\n",
    "            colormap='viridis',\n",
    "            min_font_size=10,\n",
    "            max_font_size=150,\n",
    "            random_state=42\n",
    "        ).generate_from_frequencies(topics)\n",
    "\n",
    "    # Create a figure for the word cloud\n",
    "    fig, ax = plt.subplots(figsize=(width/100, height/100))\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def bar_chart(github_data):\n",
    "    # Prepare list to store visualization images\n",
    "    # Get topic data\n",
    "    top_repos_details = github_data[\"repositories\"][\"top_repos\"]\n",
    "    print(top_repos_details)\n",
    "    fig = px.bar(\n",
    "        top_repos_details,\n",
    "        x='name',\n",
    "        y='stars',\n",
    "        color='language',\n",
    "        title='Top 10 Repositories by Stars',\n",
    "        labels={'name': 'Repository Name', 'stars': 'Number of Stars'}\n",
    "    )\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "    return fig\n",
    "\n",
    "def pie_chart(github_data):\n",
    "    repos_info = github_data[\"repositories\"][\"repos_info\"]\n",
    "    fig_language = px.pie(\n",
    "        repos_info,\n",
    "        names='language',\n",
    "        title='Repository Language Distribution',\n",
    "        hole=0.3,\n",
    "        color_discrete_sequence=px.colors.qualitative.Bold\n",
    "    )\n",
    "    fig_language.update_traces(textposition='inside', textinfo='percent+label')\n",
    "    return fig_language\n",
    "\n",
    "def time_line(github_data):\n",
    "    repos_info = github_data[\"repositories\"][\"repos_info\"]\n",
    "    df=pd.DataFrame(repos_info)\n",
    "    df['year_month'] = df['created_at'].dt.strftime('%Y-%m')\n",
    "    timeline_data = df.groupby('year_month').size().reset_index(name='count')\n",
    "    timeline_data = timeline_data.sort_values('year_month')\n",
    "\n",
    "    fig_timeline = px.line(\n",
    "        timeline_data,\n",
    "        x='year_month',\n",
    "        y='count',\n",
    "        markers=True,\n",
    "        title='Repository Creation Timeline',\n",
    "        labels={'year_month': 'Year-Month', 'count': 'Number of Repositories Created'}\n",
    "    )\n",
    "    fig_timeline.update_xaxes(tickangle=45)\n",
    "    return fig_timeline\n",
    "\n",
    "\n",
    "# Function to analyze GitHub profile using model\n",
    "def analyze_github_profile(username,model, tokenizer):\n",
    "    if not username:\n",
    "        return \"Please enter a GitHub username.\"\n",
    "\n",
    "    # Fetch and prepare data\n",
    "    github_data = prepare_github_data(username)\n",
    "    if isinstance(github_data, str):  # Error message\n",
    "        return github_data\n",
    "\n",
    "    # Create prompt for analysis\n",
    "    prompt = create_analysis_prompt(github_data)\n",
    "\n",
    "    # Generate analysis using model\n",
    "    print(\"Generating analysis with Model...\")\n",
    "    analysis = generate_with_model(model, tokenizer, prompt)\n",
    "    repo_table = format_repo_list(github_data)\n",
    "    word_cloud_output_figure = word_cloud_output(github_data)\n",
    "\n",
    "    bar_chart_figure=bar_chart(github_data)\n",
    "    pie_chart_figure = pie_chart(github_data)\n",
    "    time_line_figure = time_line(github_data)\n",
    "    # Format the final output\n",
    "    profile_url = github_data[\"profile\"][\"profile_url\"]\n",
    "    avatar_url = github_data[\"profile\"][\"avatar_url\"]\n",
    "\n",
    "    # Prepare the response with markdown\n",
    "    response = f\"\"\"# GitHub Profile Analysis for [{github_data[\"profile\"][\"username\"]}]({profile_url})\n",
    "\n",
    "![Profile Avatar]({avatar_url})\n",
    "{analysis}\n",
    "{repo_table}\n",
    "\n",
    "---\n",
    "*Analysis generated using model*\n",
    "\"\"\"\n",
    "    return response, word_cloud_output_figure, bar_chart_figure, pie_chart_figure, time_line_figure\n",
    "\n",
    "def create_gradio_interface(model, tokenizer):\n",
    "    def analyze_profile(username):\n",
    "        return analyze_github_profile(username,model, tokenizer)\n",
    "\n",
    "    with gr.Blocks(css=\".orange-button { background-color: orange !important; color: white !important; }\",title=\"GitHub Profile Analyzer\") as iface:\n",
    "        gr.Markdown(\"## GitHub Profile Analyzer \\n Enter a GitHub username to analyze their profile and repositories\")\n",
    "\n",
    "        with gr.Column():\n",
    "            username_input = gr.Textbox(lines=1, placeholder=\"Enter GitHub username\", label=\"GitHub Username\")\n",
    "            analyze_button = gr.Button(\"Analyze\",elem_classes=\"orange-button\")\n",
    "\n",
    "        output = gr.Markdown(label=\"Analysis\")\n",
    "\n",
    "        with gr.Row():\n",
    "          with gr.Column():\n",
    "              word_cloud = gr.Plot(label=\"Word Cloud\")\n",
    "          with gr.Column():\n",
    "              bar_chart_figure = gr.Plot(label=\"Bar Chart\")\n",
    "\n",
    "        with gr.Row():\n",
    "          with gr.Column():\n",
    "            pie_chart_figure = gr.Plot(label=\"Language Distribution\")\n",
    "          with gr.Column():\n",
    "            time_line_figure = gr.Plot(label=\"Repository Creation Timeline\")\n",
    "\n",
    "        analyze_button.click(fn=analyze_profile, inputs=username_input, outputs=[output, word_cloud, bar_chart_figure, pie_chart_figure, time_line_figure])\n",
    "\n",
    "    return iface\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Load the Llama 3 model\n",
    "    print(\"Starting GitHub Analyzer...\")\n",
    "    print(\"This will download and load the model (this might take a few minutes)...\")\n",
    "\n",
    "    # Check if HF_TOKEN is set for accessing gated models\n",
    "    if \"HF_TOKEN\" not in os.environ:\n",
    "        print(\"Note: HF_TOKEN is not set. You may need to set it to access the models.\")\n",
    "        print(\"Run the following in a previous cell:\")\n",
    "        print(\"import os\")\n",
    "        print(\"os.environ['HF_TOKEN'] = 'your_huggingface_token_here'\")\n",
    "    # Load model\n",
    "    model, tokenizer = load_model()\n",
    "    # Step 2: Create and launch Gradio interface\n",
    "    iface = create_gradio_interface(model, tokenizer)\n",
    "    iface.launch(debug=True,share=True)  # share=True creates a public link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
